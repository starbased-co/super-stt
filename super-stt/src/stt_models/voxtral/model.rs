// SPDX-License-Identifier: GPL-3.0-only
use std::path::PathBuf;

use anyhow::{Context, Error, Result};
use candle_core::{DType, Device, Tensor, utils};
use candle_nn::VarBuilder;
use candle_transformers::models::voxtral::{
    VoxtralCache, VoxtralConfig, VoxtralEncoderConfig, VoxtralForConditionalGeneration,
    VoxtralGenerationConfig, VoxtralLlamaConfig, audio,
};
use log::{debug, info, warn};
use serde_json;
use tekken::Tekkenizer;

use byteorder::{LittleEndian, ReadBytesExt};
use std::io::Cursor;
use super_stt_shared::{
    stt_model::STTModel,
    utils::audio::{ResampleQuality, resample},
};

const SAMPLE_RATE: u32 = 16000;

#[derive(Debug, serde::Serialize)]
pub struct TranscriptionResult {
    pub text: String,
    pub tokens: Vec<u32>,
}

pub struct VoxtralModel {
    model: VoxtralForConditionalGeneration,
    tokenizer: Tekkenizer,
    device: Device,
    config: VoxtralConfig,
    audio_token_id: usize,
    cache: VoxtralCache,
}

impl VoxtralModel {
    /// # Errors
    ///
    /// Returns an error if the model cannot be loaded.
    ///
    /// # Panics
    ///
    /// Panics if expected file names are not valid UTF-8 or missing
    /// when inspecting cached paths (due to `unwrap()` on file names).
    pub fn new(stt_model: &STTModel, force_cpu: bool) -> Result<Self> {
        info!("Loading Voxtral {stt_model:?} model...");

        // Determine device
        let device = if !force_cpu && utils::cuda_is_available() {
            info!("Using CUDA device");
            Device::new_cuda(0).context("Failed to create CUDA device")?
        } else {
            if force_cpu {
                info!("Using CPU (forced by user)");
            } else {
                info!("Using CPU (CUDA not available)");
            }
            Device::Cpu
        };

        // Get file paths from the unified download system
        let file_paths = crate::stt_models::download::get_model_file_paths(stt_model)?;

        // Extract the specific files we need
        let config_path = file_paths
            .iter()
            .find(|p| p.file_name().unwrap().to_str().unwrap() == "config.json")
            .ok_or_else(|| anyhow::anyhow!("config.json not found"))?;
        let tokenizer_path = file_paths
            .iter()
            .find(|p| p.file_name().unwrap().to_str().unwrap() == "tekken.json")
            .ok_or_else(|| anyhow::anyhow!("tekken.json not found"))?;

        // Get all safetensors files
        let safetensors_files: Vec<PathBuf> = file_paths
            .iter()
            .filter(|p| p.extension().and_then(|s| s.to_str()) == Some("safetensors"))
            .cloned()
            .collect();

        info!("Model files loaded from cache successfully");

        // Load model configuration
        info!("Loading model configuration...");
        let config = load_model_config(config_path)?;

        // Load safetensors files
        info!("Loading model weights from safetensors...");
        let vb = load_model_weights(&safetensors_files, &device)?;

        // Create model
        info!("Creating Voxtral model...");
        debug!("Config: {config:?}");
        let model = VoxtralForConditionalGeneration::new(&config, vb)?;

        // Load tokenizer
        info!("Loading tokenizer...");
        let tokenizer = Tekkenizer::from_file(tokenizer_path).map_err(Error::msg)?;

        debug!("Loaded tokenizer");
        // Create cache
        let cache = VoxtralCache::new(true, DType::F16, &config.text_config, &device)?;

        let mel_bytes = include_bytes!("../data/melfilters128.bytes").as_slice();
        let mut mel_filters = vec![0f32; mel_bytes.len() / 4];
        let mut cursor = Cursor::new(mel_bytes);
        cursor.read_f32_into::<LittleEndian>(&mut mel_filters)?;

        info!("Voxtral model loaded successfully");
        info!("Model device: {device:?}");

        let audio_token_id = config.audio_token_id;

        Ok(Self {
            model,
            tokenizer,
            device,
            config,
            audio_token_id,
            cache,
        })
    }

    /// Transcribe audio and return both text and tokens
    ///
    /// # Errors
    ///
    /// Returns an error if the audio data cannot be transcribed.
    pub fn transcribe_audio_with_tokens(
        &mut self,
        audio_data: &[f32],
        sample_rate: u32,
    ) -> Result<TranscriptionResult> {
        let (transcription, tokens) = self.transcribe_audio_internal(audio_data, sample_rate)?;

        Ok(TranscriptionResult {
            text: transcription,
            tokens,
        })
    }

    /// # Errors
    ///
    /// Returns an error if the audio data cannot be transcribed.
    pub fn transcribe_audio(&mut self, audio_data: &[f32], sample_rate: u32) -> Result<String> {
        let (transcription, _) = self.transcribe_audio_internal(audio_data, sample_rate)?;
        Ok(transcription)
    }

    /// Internal transcribe method that returns both text and tokens
    ///
    /// # Errors
    ///
    /// Returns an error if the audio data cannot be transcribed.
    fn transcribe_audio_internal(
        &mut self,
        audio_data: &[f32],
        sample_rate: u32,
    ) -> Result<(String, Vec<u32>)> {
        // Resample to 16kHz if needed
        let audio = if sample_rate == SAMPLE_RATE {
            audio_data.to_vec()
        } else {
            warn!("Audio sample rate is {sample_rate}Hz, resampling to {SAMPLE_RATE}Hz");
            resample(audio_data, sample_rate, SAMPLE_RATE, ResampleQuality::Fast)?
        };

        debug!("Converting audio to mel spectrogram using exact Whisper processing...");
        debug!(
            "Input audio length: {} samples at {}Hz",
            audio.len(),
            SAMPLE_RATE
        );

        // Debug input audio statistics
        let audio_min = audio.iter().fold(f32::INFINITY, |a, &b| a.min(b));
        let audio_max = audio.iter().fold(f32::NEG_INFINITY, |a, &b| a.max(b));
        let audio_sum: f32 = audio.iter().sum();
        #[allow(clippy::cast_precision_loss)]
        let audio_mean = audio_sum / audio.len() as f32;

        debug!(
            "Input audio stats - Min: {audio_min:.6}, Max: {audio_max:.6}, Mean: {audio_mean:.6}"
        );

        // CRITICAL: VoxtralProcessor pads audio to multiple of 480000 samples before WhisperFeatureExtractor
        // This matches VoxtralProcessorKwargs pad_to_multiple_of: 480000
        let chunk_size = 480_000; // 30 seconds * 16000 Hz
        let padded_audio = if audio.len() % chunk_size != 0 {
            // Pad to next multiple of chunk_size
            let target_samples = ((audio.len() / chunk_size) + 1) * chunk_size;
            let mut padded = audio.clone();
            padded.resize(target_samples, 0.0); // Pad with zeros
            padded
        } else {
            audio
        };

        // Use the 128-mel filter bank
        let mel_bytes = include_bytes!("../data/melfilters128.bytes");

        let mut mel_filters = vec![0f32; mel_bytes.len() / 4];
        let mut cursor = Cursor::new(mel_bytes);
        cursor.read_f32_into::<LittleEndian>(&mut mel_filters)?;

        let audio_features = audio::extract_features(&padded_audio, &mel_filters, &self.device)?;

        debug!(
            "Audio features shape after Python-style chunking: {:?}",
            audio_features.dims()
        );

        let (result, tokens) = transcribe_with_voxtral(
            &self.model,
            &self.tokenizer,
            &audio_features,
            self.audio_token_id,
            &self.device,
            &self.cache.clone(),
        )?;

        debug!("Transcription result: {result}");
        Ok((result, tokens))
    }

    #[must_use]
    pub fn device(&self) -> &Device {
        &self.device
    }

    #[must_use]
    pub fn config(&self) -> &VoxtralConfig {
        &self.config
    }
}

/// Post-process transcription to clean up formatting artifacts
///
/// This function handles common formatting issues that arise from different token
/// generation between Python and Rust implementations, particularly when the first
/// token is a quote character instead of regular text.
///
/// # Errors
///
/// Returns an error if the transcription is invalid (empty or just punctuation).
pub fn post_process_transcription(text: &str) -> Result<String> {
    let mut cleaned = text.trim().to_string();

    // Handle the case where transcription starts with quotes and has extra spaces
    // Pattern: "' It  is  a  le av ened..." -> "it is a leavened..."
    if cleaned.starts_with("\"'") || cleaned.starts_with("'\"") {
        // Remove leading quotes
        cleaned = cleaned
            .trim_start_matches("\"'")
            .trim_start_matches("'\"")
            .trim()
            .to_string();
    }

    // Remove single quotes at the beginning if present
    if cleaned.starts_with('\'') {
        cleaned = cleaned[1..].trim().to_string();
    }

    // Fix excessive spacing between words (multiple spaces to single space)
    // This handles cases like "It  is  a  le av ened" -> "It is a leavened"
    cleaned = cleaned.split_whitespace().collect::<Vec<_>>().join(" ");

    // Fix split words that should be joined
    // Common patterns from Voxtral output
    let word_fixes = [
        ("le av ened", "leavened"),
        ("smile ware", "smileware"),
        ("del ved", "delved"),
        ("fra il", "frail"),
        ("N ay", "Nay"),
        ("N oring", "Noring"),
    ];

    for (pattern, replacement) in &word_fixes {
        cleaned = cleaned.replace(pattern, replacement);
    }

    // Remove quote patterns in the middle of text
    cleaned = cleaned.replace(" \"' ", " ");
    cleaned = cleaned.replace(" '\" ", " ");

    // Handle case where Rust mel generation produces just "."
    if cleaned == "." || cleaned.trim().is_empty() {
        return Err(anyhow::anyhow!(
            "Mel feature generation produced invalid output. This is a known issue with Candle's mel spectrogram implementation."
        ));
    }

    // Remove any trailing quotes
    cleaned = cleaned
        .trim_end_matches('\'')
        .trim_end_matches('"')
        .to_string();

    Ok(cleaned)
}

fn transcribe_with_voxtral(
    model: &VoxtralForConditionalGeneration,
    tokenizer: &Tekkenizer,
    audio_features: &Tensor,
    audio_token_id: usize,
    device: &Device,
    cache: &VoxtralCache,
) -> Result<(String, Vec<u32>)> {
    debug!("Audio features shape: {:?}", audio_features.dims());
    debug!("Using audio_token_id: {audio_token_id}");

    // Validate audio features shape
    let audio_dims = audio_features.dims();
    if audio_dims.len() != 3 {
        return Err(anyhow::anyhow!(
            "Audio features must be 3D tensor (batch, mels, time), got shape: {:?}",
            audio_dims
        ));
    }

    if audio_dims[1] != 128 {
        return Err(anyhow::anyhow!(
            "Audio features must have 128 mel bins, got {}",
            audio_dims[1]
        ));
    }

    debug!("Audio features validation passed");

    // Create the exact token sequence that HuggingFace processor generates
    let mut input_tokens = Vec::new();

    // Pattern: <s>[INST][BEGIN_AUDIO][AUDIO]*N[/INST]lang:en[TRANSCRIBE]
    input_tokens.push(1u32); // BOS: <s>
    input_tokens.push(3u32); // [INST]
    input_tokens.push(25u32); // [BEGIN_AUDIO]

    // Calculate number of audio tokens to match Python exactly: 7 chunks × 375 tokens = 2625
    let batch_size = audio_features.dim(0)?; // Number of chunks (should be 7)
    let frames_per_chunk = audio_features.dim(2)?; // Should be 3000
    debug!("Audio features: {batch_size} chunks, {frames_per_chunk} frames per chunk");

    // Python uses exactly 375 tokens per 3000-frame chunk
    let tokens_per_chunk = 375; // Fixed value from Python analysis
    let num_audio_tokens = batch_size * tokens_per_chunk;
    debug!(
        "Using {num_audio_tokens} audio tokens ({batch_size} chunks × {tokens_per_chunk} tokens per chunk)"
    );

    // Add AUDIO tokens
    for _ in 0..num_audio_tokens {
        #[allow(clippy::cast_possible_truncation)]
        {
            input_tokens.push(audio_token_id as u32); // [AUDIO] token (24)
        }
    }

    input_tokens.push(4u32); // [/INST]
    input_tokens.push(9909u32); // lang
    input_tokens.push(1058u32); // :
    input_tokens.push(1262u32); // en
    input_tokens.push(34u32); // [TRANSCRIBE]

    debug!("=== RUST PROCESSING DEBUG ===");
    debug!("Total tokens: {}", input_tokens.len());
    debug!("Audio token count (24s): {num_audio_tokens}");
    debug!(
        "First 10 tokens: {:?}",
        &input_tokens[..input_tokens.len().min(10)]
    );
    debug!(
        "Last 10 tokens: {:?}",
        &input_tokens[input_tokens.len().saturating_sub(10)..]
    );

    let input_len = input_tokens.len();
    let input_ids = Tensor::new(input_tokens, device)?.unsqueeze(0)?;

    // Calculate approximate memory usage
    let _input_elements = input_ids.dims().iter().product::<usize>();
    let _audio_elements = audio_features.dims().iter().product::<usize>();

    let config = VoxtralGenerationConfig {
        max_new_tokens: 1000,
        temperature: 0.0,
        top_p: None,
        device: device.clone(),
        cache: Some(cache.clone()),
    };

    // Generate response using the model (match Python parameters)
    debug!("About to call model.generate()");
    let generated_tokens = model
        .generate(
            &input_ids,
            Some(audio_features), // Audio features will be processed and inserted at audio token position
            config,
        )
        .map_err(|e| {
            debug!("Generation error: {e:?}");
            debug!("Error details: {e:#}");
            anyhow::anyhow!("Failed to generate tokens: {}", e)
        })?;

    // Decode only the newly generated tokens (skip input prompt)
    let new_tokens = if generated_tokens.len() > input_len {
        &generated_tokens[input_len..]
    } else {
        &generated_tokens
    };

    debug!("=== RUST TOKEN OUTPUT DEBUG ===");
    debug!("Total new tokens generated: {}", new_tokens.len());
    debug!("Full token list: {new_tokens:?}");
    debug!("First 30 tokens with positions:");
    for (i, &token_id) in new_tokens.iter().take(30).enumerate() {
        debug!("  {i:2}: {token_id}");
    }

    let decoded_text = tokenizer
        .decode(new_tokens, tekken::SpecialTokenPolicy::Ignore)
        .map_err(|e| anyhow::anyhow!("Failed to decode tokens: {}", e))?;

    debug!("Full decoded text: {decoded_text}");

    // Post-process the transcription to clean up formatting artifacts
    let transcription = post_process_transcription(&decoded_text)?;

    debug!("Final transcription: {transcription}");

    // Return both transcription and tokens
    Ok((transcription, new_tokens.to_vec()))
}

/// Load model weights from safetensors files
fn load_model_weights<'a>(model_files: &'a [PathBuf], device: &Device) -> Result<VarBuilder<'a>> {
    let dtype = DType::F16; // F16 for memory efficiency

    info!("Loading {} safetensors files...", model_files.len());
    for file in model_files {
        info!("  - {}", file.display());
    }

    let vb = unsafe { VarBuilder::from_mmaped_safetensors(model_files, dtype, device)? };

    Ok(vb)
}

/// Load model configuration from JSON file
fn load_model_config(config_file: &PathBuf) -> Result<VoxtralConfig> {
    let config_str = std::fs::read_to_string(config_file)?;

    // Parse the JSON configuration
    let json: serde_json::Value =
        serde_json::from_str(&config_str).context("Failed to parse config.json")?;

    // Extract audio token ID (should be 24 based on config.json)
    let audio_token_id = json
        .get("audio_token_id")
        .and_then(serde_json::Value::as_u64)
        .and_then(|v| usize::try_from(v).ok())
        .unwrap_or(24);

    debug!("Using audio_token_id: {audio_token_id}");

    // Parse audio config from JSON
    let audio_config = parse_audio_config(&json)?;

    // Parse text config from JSON
    let text_config = parse_text_config(&json)?;

    // Get projector activation function
    let projector_hidden_act = json
        .get("projector_hidden_act")
        .and_then(|v| v.as_str())
        .unwrap_or("gelu")
        .to_string();

    Ok(VoxtralConfig {
        audio_config,
        text_config,
        audio_token_id,
        projector_hidden_act,
    })
}

/// Parse audio encoder config from JSON
fn parse_audio_config(json: &serde_json::Value) -> Result<VoxtralEncoderConfig> {
    let audio_json = json
        .get("audio_config")
        .ok_or_else(|| anyhow::anyhow!("Missing audio_config in configuration"))?;

    Ok(VoxtralEncoderConfig {
        vocab_size: audio_json
            .get("vocab_size")
            .and_then(serde_json::Value::as_u64)
            .and_then(|v| usize::try_from(v).ok())
            .unwrap_or(51866),
        hidden_size: audio_json
            .get("hidden_size")
            .and_then(serde_json::Value::as_u64)
            .and_then(|v| usize::try_from(v).ok())
            .unwrap_or(1280),
        num_hidden_layers: audio_json
            .get("num_hidden_layers")
            .and_then(serde_json::Value::as_u64)
            .and_then(|v| usize::try_from(v).ok())
            .unwrap_or(32),
        num_attention_heads: audio_json
            .get("num_attention_heads")
            .and_then(serde_json::Value::as_u64)
            .and_then(|v| usize::try_from(v).ok())
            .unwrap_or(20),
        num_key_value_heads: audio_json
            .get("num_key_value_heads")
            .and_then(serde_json::Value::as_u64)
            .and_then(|v| usize::try_from(v).ok())
            .unwrap_or(20),
        intermediate_size: audio_json
            .get("intermediate_size")
            .and_then(serde_json::Value::as_u64)
            .and_then(|v| usize::try_from(v).ok())
            .unwrap_or(5120),
        dropout: audio_json
            .get("dropout")
            .and_then(serde_json::Value::as_f64)
            .unwrap_or(0.0),
        attention_dropout: audio_json
            .get("attention_dropout")
            .and_then(serde_json::Value::as_f64)
            .unwrap_or(0.0),
        activation_dropout: audio_json
            .get("activation_dropout")
            .and_then(serde_json::Value::as_f64)
            .unwrap_or(0.0),
        activation_function: audio_json
            .get("activation_function")
            .and_then(|v| v.as_str())
            .unwrap_or("gelu")
            .to_string(),
        max_source_positions: audio_json
            .get("max_source_positions")
            .and_then(serde_json::Value::as_u64)
            .and_then(|v| usize::try_from(v).ok())
            .unwrap_or(1500),
        layerdrop: audio_json
            .get("layerdrop")
            .and_then(serde_json::Value::as_f64)
            .unwrap_or(0.0),
        initializer_range: audio_json
            .get("initializer_range")
            .and_then(serde_json::Value::as_f64)
            .unwrap_or(0.02),
        scale_embedding: audio_json
            .get("scale_embedding")
            .and_then(serde_json::Value::as_bool)
            .unwrap_or(false),
        num_mel_bins: audio_json
            .get("num_mel_bins")
            .and_then(serde_json::Value::as_u64)
            .and_then(|v| usize::try_from(v).ok())
            .unwrap_or(128),
        head_dim: audio_json
            .get("head_dim")
            .and_then(serde_json::Value::as_u64)
            .and_then(|v| usize::try_from(v).ok())
            .unwrap_or(64),
    })
}

#[cfg(feature = "flash-attn")]
const fn use_flash_attn() -> bool {
    true
}

#[cfg(not(feature = "flash-attn"))]
const fn use_flash_attn() -> bool {
    false
}

/// Parse text model config from JSON
fn parse_text_config(json: &serde_json::Value) -> Result<VoxtralLlamaConfig> {
    let text_json = json
        .get("text_config")
        .ok_or_else(|| anyhow::anyhow!("Missing text_config in configuration"))?;

    let use_flash_attn = use_flash_attn();
    if use_flash_attn {
        log::info!("Using flash attention");
    } else {
        log::info!("Not using flash attention");
    }

    Ok(VoxtralLlamaConfig {
        vocab_size: text_json
            .get("vocab_size")
            .and_then(serde_json::Value::as_u64)
            .and_then(|v| usize::try_from(v).ok())
            .unwrap_or(131_072),
        hidden_size: text_json
            .get("hidden_size")
            .and_then(serde_json::Value::as_u64)
            .and_then(|v| usize::try_from(v).ok())
            .unwrap_or(3072),
        intermediate_size: text_json
            .get("intermediate_size")
            .and_then(serde_json::Value::as_u64)
            .and_then(|v| usize::try_from(v).ok())
            .unwrap_or(8192),
        num_hidden_layers: text_json
            .get("num_hidden_layers")
            .and_then(serde_json::Value::as_u64)
            .and_then(|v| usize::try_from(v).ok())
            .unwrap_or(30),
        num_attention_heads: text_json
            .get("num_attention_heads")
            .and_then(serde_json::Value::as_u64)
            .and_then(|v| usize::try_from(v).ok())
            .unwrap_or(32),
        num_key_value_heads: text_json
            .get("num_key_value_heads")
            .and_then(serde_json::Value::as_u64)
            .and_then(|v| usize::try_from(v).ok())
            .unwrap_or(8),
        head_dim: text_json
            .get("head_dim")
            .and_then(serde_json::Value::as_u64)
            .and_then(|v| usize::try_from(v).ok()),
        rms_norm_eps: text_json
            .get("rms_norm_eps")
            .and_then(serde_json::Value::as_f64)
            .unwrap_or(1e-5),
        // Convert to f32 for model config; truncation is acceptable here
        rope_theta: {
            let v = text_json
                .get("rope_theta")
                .and_then(serde_json::Value::as_f64)
                .unwrap_or(100_000_000.0);
            #[allow(clippy::cast_possible_truncation)]
            {
                v as f32
            }
        },
        max_position_embeddings: text_json
            .get("max_position_embeddings")
            .and_then(serde_json::Value::as_u64)
            .and_then(|v| usize::try_from(v).ok())
            .unwrap_or(131_072),
        use_flash_attn,
        tie_word_embeddings: text_json
            .get("attention_bias")
            .and_then(serde_json::Value::as_bool)
            .unwrap_or(false),
    })
}
